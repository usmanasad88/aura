I want to create a generic theoretical and programmatic framework for proactive robotic assistance. Here are the main ideas: Frontier AI models exhibit incredible reasoning skills in text based tasks, and are getting increasingly good at processing multi-modal data including vision and sound. Interaction with these agents are mostly 'turn-based': a series of prompts and a responses. These are costly time-consuming API calls. Other modern perception and control tools can perform more in realtime, for example to segment masks for arbitrary objects, or to track human or object pose in real time. Frontier AI models have the ability to reason about this when prompted correctly, at the right time, with the right information. Additionally, we are starting to see VLAs to control robot behavior given text instructions and the state of the system (usually just RGB images) that can be used to output 7-DOF end-effector motion (for manipulators) or higher-dimensional output for humanoids with action chunking. The performance of these models is somewhat brittle, as they usually work for a narrow set of "skills" under similar environmental conditions. However, they can be fine-tuned to learn new tasks if data is collected for the target task and environment. For certain easier tasks, it may be convenient to just use motion primitives. i.e. to command the end-effector to move to a target location, close or close gripper, or follow a trajectory, or plan a collision free path. We have libraries such as cuRobo to generate paths for a given robot + obstacles. The major limitation of research in this domain is that the task needs to be explicitly provided to the model. We're trying to create a need-based proactive assistance framework instead, that is agentic in nature, and can call the necessary tools to be useful, given a shared overall 'goal'. We want to limit the overall scope, by saying the SOPs for the overall task or set of possible tasks should be known. As a motivating example, and a playground to test algorithms, I'm working on a collaborative game. Where there is a human agent and an AI agents, which are just circles in a pygame. There are also a number of objects and two target zones. This may be considered a surrogate for a sorting/packing/warehouse logistic problem. The overall goal can be known to both the AI and the human. This may be provided to the system as the SOP, which can include a lot of the instructions for the AI agent. For example, for the sorting task: "Sort the objects in the boxes as instructed by the human operator. In general, move the larger boxes before the smaller ones. Ensure that the boxes are entirely inside the designated zones. The human and AI will work together is roughly the same space."

these are my thoughts about the task flow, and what that translates into for our motivating example
First the AI detect the human intention (first clarify how the boxes are to be sorted, and then figure out which box the human is trying to move) \to Predicts future human pose (or the future position of the circle) \to Decide what should the robot do (First perceive the scene and judge it's own capabilities of what the robot can do... next, if the human is not doing anything, perhaps the robot can start moving any box as per convenience. i.e. probably move the box closest to the robot. If the human is moving a box, the robot should stay away from the expected trajectory of the human and the box the human operator is moving) \to Decide whether to ask or execute. If the humans actions don't conform to the SOP or for example the human leaves or does something unexpected or wrong, then the robot should ask for clarification. If the robot is confident, it can move directly to execution, otherwise it can ask the human what it should do (Confirm if it should move a particular box to a particular place. Here programatically, there should be something telling the AI agent what modalities are available to communicate with the human, i.e. by speech, by printing a message or by visualizing the motion in a digital twin). After executing or communicating, it can determine if the tasks is completed. If not, it can continue the loop. During this loop, it should constantly monitor the human actions, the audio input, and the task state. Another case study I want to include after the motivating example is a lab-scale representation of composite layup process. Here the overall tasks can be for example to place a mold on the table. Mix epoxy and hardener. Measure it in a beaker. Give the tools to the operator. The human takes pre-cut pieces of fiber glass. Places it on the mold, applies the resin. The robot inspects the progress. Checks that there are no wrinkles or flaws in the layup, and picks and places objects as needed by the human.

The collaborative game is an example demo where I want to apply the framework to test its effectiveness. These are instructions for a previous impelementation of a Collaborative game: /home/mani/Repos/proactive_hcdt/tests/COLLABORATIVE_GAME_README.md

I'm trying to modularize and make the AI based perception and decision making explainable for proactive assistance. It's not enough for the robot to make the correct decision. The reason for choosing a certain action should also be clear. . My pipeline overall consists of the following nodes which inform the 'brain':
1. An Intent Monitor (What is the human attempting to do?)
2. Motion Predictor (How is the human (or other objects) predicted to move) This is previously accomplished using the run_phase_two.py script for different tasks.
3. The Perception Module: To initialize the project, let's use just an Object Mask Monitor (Tracks the masks and bounding boxes of objects of interest which can queried or automatically generated. i.e. The 'brain' can ask it to query let's say a hammer, or the node can look at the frame and return masks and bboxes for important objects) The testing.py script can accomplish this already.
4. Sound Monitor: Listens to an audio stream, converts to text, and analyses it with reference to the overall objective. Ignores irrelevant speech. Otherwise forwards to the brain. We can use Gemini Live for this.
5. Affordance Monitor: Lists the actions the system can perform. For the proactive robot example, this module understands the robot's known skill like "Fold the laundy", or motion primitives, go to position X, open/close gripper. Based on that, the module can take perception data as input, and based return affordance based on the scene or a query. For example, based on the current wrist scene setting I can Pick up objects [cup, bottle]
6. Performance Monitor: The brain can issue a command. But maybe the robot is not moving, or is failing to perform the task. The module is responsible for monitoring the performance of the assigned action and reporting back to the brain

The brain can perform a sound based output (Reply to the human) or an action. A visualization system is constantly being updated based on the perception and the outputs of all nodes. A 'State' is being maintained, which can consist of:
Objects, Object Positions, Human Pose, Current Actions, Prediction Future Actions, Task State, Previous Verbal Communication, List of Possible Robot Actions, Current Robot Action, Performance Status. 

First time setup and start-up:
Requires a 3D environment model: 3DGS which can be imported in Isaac Sim. This can be done by running for example the script: 
/home/mani/Repos/Depth-Anything-3/scripts/run_scene_pipeline.sh Lounge.mp4 lounge_scene 2.0 \
  -- dataset.downsample_factor=2 export_usdz.apply_normalizing_transform=true

Next we need to create meshes for known objects of interest. This can be done using SAM3D-Objects, or InstantMesh or Hunyuan3D. However, this step will be done manually by the user, and models will be stored in an appropriate folder.
The Standard Operating Procedures will be provided in the form of a Task Graph for a particular task, in JSON format, as well as the robot's role and guidelines specified in natural language. This will be stored in an appropriate location. This may be done by an AI agent for an example task.
During start-up:
1. The Semantic Scene Graph will be generated and initialized.
2. The Robot will be initiailized
3. The Perception Models will be initialized.
4. The Human-Centric Digital TWin Visualization will be started. This can be an Isaac Sim extension (A previous example is /home/mani/isaac-sim-standalone-5.0.0-linux-x86_64/exts/isaacsim.examples.interactive/isaacsim/examples/interactive/ur10_robotiq_cortex/ur10_robotiq_cortex_extension.py)
5. The control loop will be started. Actual robot control is in a separate repo at: /home/mani/Repos/ur_ws/robot_interface.py

For memory management. Use: 
LangGraph Checkpointer (Short-term): Use LangGraph's built-in MemorySaver (Postgres/SQLite) to remember the current conversation and task state (last 5 minutes).
ChromaDB (Long-term): Use a local vector store to save "Lessons Learned."
Create a "Reflection Node" in your graph. After every task, the LLM summarizes: "What went wrong?" and saves that summary to ChromaDB.
At the start of the next task, it queries ChromaDB for advice.

The system should have something like an interrupt. Let's say I want to monitor the presence of humans in a kitchen, and stop all robot motion if a human is detected. This needs to be flexible because there can be other conditions that can cause Gemini to change course, for example a voice command

My project has a lot of possibly conflicting dependencies. I'll need sam3 and sam3d-body for perception along with depth anything or a depth model for panoramic pictures. This is all just perception. I need Isaac Sim for visualization of the digital twin, ROS for physical robot control. I need LangGraph for agentic control. I need a web based UI for monitoring the system state, and execution flow. Some tools should be exposed as MCP to a Google genai agent.


This is a large project that cannot be completed by an AI agent in one prompt. First modularize the tasks and create a plan for AI agents to work on specific chunks. We want to use LangGraph, ROS nodes, MCP servers, MQTT (if needed), Google A2A protocol
Write the plans to the /home/mani/Repos/aura/genai_instructions folder. We want to create "aura" as the master Github Repo. Necessary models can be copied to third_party. Scripts outside the Repo can be used as reference to write new optimized code.

Investigate how the Aura framework and implement this. Plan and ask before implementation:
Formalizing the State: The Semantic Scene Graph (SSG)
Instead of a flat list of variables, formalize your state into a Dynamic Spatio-Temporal Scene Graph. This becomes the "Shared Truth" that all nodes read from and write to.

Nodes: Objects (for example bottle, hammer, cup), Agents (Human, Robot), and Regions (Table, Bin).

Edges: Spatial relations (On, Near, Inside) and Semantic relations (OwnedBy, TargetedBy).

Attributes: State (Dirty, Full, Empty), Affordances (Pickable, Foldable), and Predicted Action (WillMoveTo).

Why this helps Explainability: When the Brain makes a decision, it can cite a specific edge in the graph. "I moved the hammer because edge [Human] --Target--> [Hammer] was predicted, but [Box] --Blocks--> [Hammer] was true."


Consider two demo tasks, Let's consider an example of making tea. The objects of interest can include the stove, the pan, the water, the cups, the spoon, tea, milk, sugar containers. They would all essentially be on the same surface, except maybe the spoon can be in a drawer. We can perhaps find the camera-frame coordinates or world-frame cordinates or find masks and bbox for them for particular input images. From a decision making perspective, the SOP for making tea will be created and available in the state.json and dag.json files and the yaml files. Lets say the state contains booleans like water added to pot, water boiling, tea added to boiling water, milk added to cup. Let's say the robot affordance includes a learned skill for adding sugar to the cup if the cup and sugar are in place and reachable. So the system should have the necessary expressiveness to say that, water was on the pot, the cup was in position, the sugar was not added, the robot is capable of adding sugar, so the robot should add sugar. 

The second illustrative task is robot assisted weighing of resin and hardner bottle. Here two bottles are placed on one table and a human with a weight machine is sitting on the other table. First The hardener bottle is picked, then the human weighs it. Then the resin bottle is picked then the user weighs it. Then the hardener and resin bottles are returned. The next action is taken after the robot returns to its home position.  The ground truth actions are recorded in the wizard of oz experiment. Along with the timings. The 3rd person video (video.mp4) and the robot gripper perspective 360 degree GoPro video (exp.mp4) are available in the demo_data folder. We don't want to hard-code specific examples, instead we want to rely on the intelligence of frontier AI models taking input from gesture, object detection and segementation models.
The base Semantic Scene Graph is created which is generic and not task specific. There is a separate task specific folder, that can essentially be deleted without consequence to the base framework, for the task specific implementation. 

For the bottle weighing task, the system should is able to read the frames and audio from input videos. The perception module should identify important objects. The intention module should detect the current and expected future actions. The prediction module should predict future hand states. The sound module should check for any speech. Based on all monitors, the decision engine should invoke which robot skills to execute at what time. 

While modifying the existing monitors, ensure that they remain generic. Create new files for task specific monitors if needed
