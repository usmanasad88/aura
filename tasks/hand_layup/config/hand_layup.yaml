# AURA Configuration for Hand Layup Task
# UR5 + Robotiq 2F-85 assisting with fiberglass hand layup process

demo:
  name: "hand_layup"
  description: "Robot assists human with fiberglass hand layup by moving objects and monitoring process"
  sop_file: "tasks/hand_layup/config/hand_layup.json"
  demo_data_dir: "demo_data/layup_demo"

# Data files for dummy demo
data:
  video_3rd_person: "demo_data/layup_demo/layup_dummy_demo_crop_1080.mp4"
  reference_frame: "demo_data/layup_demo/layup_dummy_frame.jpg"

# Brain / Decision Engine
brain:
  model: "gemini-3-pro-preview"
  temperature: 0.3
  max_tokens: 4096
  reasoning_style: "chain_of_thought"
  explanation_verbosity: "high"
  
  # Task-specific decision parameters
  proactive_assistance: true
  anticipation_horizon_seconds: 10
  verbal_announcements: true
  
  # Wait for home position before next action
  wait_for_home_position: true
  
  # Safety overrides
  pause_on_human_proximity: true
  minimum_clearance_m: 0.15
  
  # Housekeeping - proactively remove unused tools
  housekeeping_enabled: true

# Perception Pipeline  
perception:
  camera_3rd_person:
    type: "webcam"
    resolution: [1920, 1080]
    fps: 30
  
  # Object detection for layup workspace
  object_detection:
    model: "sam3"
    confidence_threshold: 0.3
    use_sam3: true
    classes:
      - "fiberglass sheet"
      - "metal mold"
      - "resin bottle"
      - "hardener bottle"
      - "cup"
      - "mixing stick"
      - "weigh scale"
      - "small brush"
      - "medium brush"
      - "roller"
      - "person"
      - "hand"
      - "gloves"
      - "table"
      - "robot arm"
    aliases:
      fiberglass: "fiberglass sheet"
      mold: "metal mold"
      resin: "resin bottle"
      hardener: "hardener bottle"
      mixing_cup: "cup"
      scale: "weigh scale"
      brush_small: "small brush"
      brush_medium: "medium brush"
  
  # Human pose for safety and intent
  human_pose:
    model: "rtmpose"
    track_hands: true
    track_gaze: true
    
  # Safety monitoring
  safety:
    gloves_detection: true
    gloves_required_phases:
      - "resin_preparation"
      - "mixing"
      - "layer_1_placement"
      - "layer_1_resin"
      - "layer_2_placement"
      - "layer_2_resin"
      - "layer_3_placement"
      - "layer_3_resin"
      - "layer_4_placement"
      - "layer_4_resin"
      - "consolidation"

# Sound Monitor
sound:
  enabled: true
  sample_rate: 16000
  
  wake_words: ["robot", "helper", "aura"]
  command_phrases:
    - "bring the {tool}"
    - "move the {tool}"
    - "I need the {tool}"
    - "remove the {tool}"
    - "clear the table"
    - "pause"
    - "continue"
    - "emergency stop"
  
  tts:
    enabled: true
    voice: "en-US-Neural2-J"
    announcements:
      - "Delivering {object_name} to workplace"
      - "Removing {object_name} from workplace"
      - "Warning: gloves not detected"
      - "Warning: mixture shelf life at {minutes} minutes"
      - "Task complete. Good work!"

# Motion Prediction
motion:
  enabled: true
  model_type: "mediapipe"
  update_rate_hz: 15.0
  prediction_horizon_seconds: 2.0
  
  poses:
    - "reaching_for_object"
    - "pouring"
    - "stirring"
    - "placing_fiberglass"
    - "brushing_resin"
    - "rolling"
    - "waiting"
    - "inspecting"

# Monitors Configuration
monitors:
  perception:
    enabled: true
    update_rate_hz: 5.0
    use_sam3: false
    use_gemini_detection: true
    max_objects: 15
  
  motion:
    enabled: true
    update_rate_hz: 15.0
    prediction_horizon_seconds: 2.0
  
  sound:
    enabled: true
    use_gemini_live: true
    wake_word_enabled: true
    wake_word: "robot"
  
  affordance:
    enabled: true
    update_rate_hz: 5.0
    use_llm: true
    # UR5 External Control API — set to connect affordance monitor to real robot
    robot_api_url: "http://localhost:5050"

  safety:
    enabled: true
    gloves_check_interval_hz: 2.0
    shelf_life_check_interval_hz: 0.5

# UR5 Robot Control (External API)
robot:
  api_url: "http://localhost:5050"
  enabled: true
  # Categories of commands to expose to the voice interface
  # Options: program, named_position_joint, named_position_pose, gripper, control
  voice_command_categories:
    - program
    - named_position_joint
    - named_position_pose
    - gripper
    - control

# Voice-to-Action Bridge (connects sound monitor → robot API)
voice_action:
  enabled: true
  system_instruction: >
    You are a robot assistant helping a human operator perform a fiberglass
    hand layup. The human does the layup work. You control a UR5 robot arm 
    with a Robotiq gripper that can move objects between the storage table 
    and the workplace table, go to named positions, run saved programs, 
    and control the gripper.
    When the human gives a clear, unambiguous robot command (e.g. "go home",
    "open the gripper", "run pick and place"), execute it IMMEDIATELY by
    calling the appropriate tool — do NOT ask for confirmation.
    Only ask for confirmation when the request is ambiguous or could match
    multiple commands. If you're unsure which command to use, call
    get_available_commands first.
    IMPORTANT SPEECH RULES:
    - Always respond by SPEAKING concisely and naturally. Never write
      markdown, headers, bullet points, or reasoning text.
    - Keep spoken responses short (1-2 sentences). For example, say
      "Done, I moved to Home" not a paragraph explaining what you did.
    - When you need clarification, ask a brief spoken question like
      "What name should I save this position as?" — do not write an
      analysis of the parameters.
    - When saving a position, note that spaces in names are converted
      to underscores (e.g. "position 1" becomes "position_1"). Use
      the underscore form when referring to saved positions.
  voice_name: "Zephyr"
  enable_speech_output: true

# Actions - Robot object-moving skills
actions:
  executor_type: "simulation"  # For offline analysis of dummy video
  action_timeout_seconds: 30.0
  verify_completion: true
  
  # Robot skills (not pre-programmed UR programs, but parameterized pick-place)
  skills:
    move_to_workplace:
      description: "Pick object from storage and place on workplace table"
      parameters: ["object_id"]
      duration_seconds: 15.0
    move_to_storage:
      description: "Pick object from workplace and place on storage table (housekeeping)"
      parameters: ["object_id"]
      duration_seconds: 15.0

# Communication
communication:
  speech_enabled: true
  text_display_enabled: true
  digital_twin_enabled: false

# Interface
interface:
  interface_type: "offline_analysis"

# Logging
logging:
  level: "DEBUG"
  log_to_file: true
  log_directory: "logs/hand_layup"

# Evaluation metrics (A-Score components from paper)
evaluation:
  enabled: true
  metrics:
    timeliness:
      enabled: true
      optimal_window_seconds: 5.0
    modality:
      enabled: true
      expected_modalities: ["deliver", "remove", "alert"]
    necessity:
      enabled: true
      track_human_requests: true
