
@Article{s25206394,
AUTHOR = {Mehta, Vinit and Sharma, Charu and Thiyagarajan, Karthick},
TITLE = {Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy},
JOURNAL = {Sensors},
VOLUME = {25},
YEAR = {2025},
NUMBER = {20},
ARTICLE-NUMBER = {6394},
URL = {https://www.mdpi.com/1424-8220/25/20/6394},
PubMedID = {41157446},
ISSN = {1424-8220},
ABSTRACT = {With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason, and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications, and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding, and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis, and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory, and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment, and real-time processing capabilities, which pave the way for more intelligent, context-aware, and autonomous robotic sensing systems.},
DOI = {10.3390/s25206394}
}

@article{David2023,
title = {Deploying OWL ontologies for semantic mediation of mixed-reality interactions for human–robot collaborative assembly},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {359-381},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001450},
author = {Joe David and Eric Coatanéa and Andrei Lobov},
keywords = {Human–robot collaboration, Mixed reality, Multi-agent systems, OWL ontology, SHACL, Belief–desire–intent, Digital thread},
abstract = {For effective human–robot collaborative assembly, it is paramount to view both robots and humans as autonomous entities in that they can communicate, undertake different roles, and not be bound to pre-planned routines and task sequences. However, with very few exceptions, most of recent research assumes static pre-defined roles during collaboration with centralised architectures devoid of runtime communication that can influence task responsibility and execution. Furthermore, from an information system standpoint, they lack the self-organisation needed to cope with today’s manufacturing landscape that is characterised by product variants. Therefore, this study presents collaborative agents for manufacturing ontology (CAMO), which is an information model based on description logic that maintains a self-organising team network between collaborating human–robot multi-agent system (MAS). CAMO is implemented using the Web Ontology Language (OWL). It models popular notions of net systems and represents the agent, manufacturing, and interaction contexts that accommodate generalisability to different assemblies and agent capabilities. As a novel element, a dynamic consensus-driven collaboration based on parametric validation of semantic representations of agent capabilities via runtime dynamic communication is presented. CAMO is instantiated as agent beliefs in a framework that benefits from real-time dynamic communication with the assembly design environment and incorporates a mixed-reality environment for use by the operator. The employment of web technologies to project scalable notions of intentions via mixed reality is discussed for its novelty from a technology standpoint and as an intention projection mechanism. A case study with a real diesel engine assembly provides appreciable results and demonstrates the feasibility of CAMO and the framework.}
}

@Article{Umbrico2022,
AUTHOR = {Umbrico, Alessandro and Orlandini, Andrea and Cesta, Amedeo and Faroni, Marco and Beschi, Manuel and Pedrocchi, Nicola and Scala, Andrea and Tavormina, Piervincenzo and Koukas, Spyros and Zalonis, Andreas and Fourtakas, Nikos and Kotsaris, Panagiotis Stylianos and Andronas, Dionisis and Makris, Sotiris},
TITLE = {Design of Advanced Human–Robot Collaborative Cells for Personalized Human–Robot Collaborations},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {14},
ARTICLE-NUMBER = {6839},
URL = {https://www.mdpi.com/2076-3417/12/14/6839},
ISSN = {2076-3417},
ABSTRACT = {Industry 4.0 is pushing forward the need for symbiotic interactions between physical and virtual entities of production environments to realize increasingly flexible and customizable production processes. This holds especially for human–robot collaboration in manufacturing, which needs continuous interaction between humans and robots. The coexistence of human and autonomous robotic agents raises several methodological and technological challenges for the design of effective, safe, and reliable control paradigms. This work proposes the integration of novel technologies from Artificial Intelligence, Control and Augmented Reality to enhance the flexibility and adaptability of collaborative systems. We present the basis to advance the classical human-aware control paradigm in favor of a user-aware control paradigm and thus personalize and adapt the synthesis and execution of collaborative processes following a user-centric approach. We leverage a manufacturing case study to show a possible deployment of the proposed framework in a real-world industrial scenario.},
DOI = {10.3390/app12146839}
}

@INPROCEEDINGS{PACE,
  author={De Lazzari, Davide and Terreran, Matteo and Giacomuzzo, Giulio and Jain, Siddarth and Falco, Pietro and Carli, Ruggero and Romeres, Diego},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={PACE: Proactive Assistance in Human-Robot Collaboration Through Action-Completion Estimation}, 
  year={2025},
  volume={},
  number={},
  pages={6725-6731},
  keywords={Tracking;Heuristic algorithms;Estimation;Collaboration;Human-robot interaction;Reinforcement learning;Real-time systems;User experience;Synchronization;Assembly},
  doi={10.1109/ICRA55743.2025.11127399}}



@article{towardsproactive,
title = {Towards proactive human–robot collaboration: A foreseeable cognitive manufacturing paradigm},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {547-552},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001540},
author = {Shufei Li and Ruobing Wang and Pai Zheng and Lihui Wang},
keywords = {Human–robot collaboration, Mass personalization, Cognitive manufacturing, Industrial Internet-of-Things, Smart manufacturing},
abstract = {Human–robot collaboration (HRC) has attracted strong interests from researchers and engineers for improved operational flexibility and efficiency towards mass personalization. Nevertheless, existing HRC development mainly undertakes either human-centered or robot-centered manner reactively, where operations are conducted by following the pre-defined instructions, thus far from an efficient integration of robotic automation and human cognitions. The prevailing research on human-level information processing of cognitive computing, the industrial IoT, and robot learning creates the possibility of bridging the gap of knowledge distilling and information sharing between onsite operators, robots and other manufacturing systems. Hence, a foreseeable informatics-based cognitive manufacturing paradigm, Proactive HRC, is introduced as an advanced form of Symbiotic HRC with high-level cognitive teamwork skills to be achieved stepwise, including: (1) inter-collaboration cognition, establishing bi-directional empathy in the execution loop based on a holistic understanding of humans and robots’ situations; (2) spatio-temporal cooperation prediction, estimating human–robot–object interaction of hierarchical sub-tasks/activities over time for the proactive planning; and (3) self-organizing teamwork, converging knowledge of distributed HRC systems for self-organization learning and task allocation. Except for the description of their technical cores, the main challenges and potential opportunities are further discussed to enable the readiness towards Proactive HRC.}
}

@inproceedings{Wang2024,
  author    = {Wang, Huaxiaoyue and Kedia, Kushal and Ren, Juntao and Abdullah, Rahma and Bhardwaj, Atiksh and Chao, Angela and Chen, Kelly Y and Chin, Nathaniel and Dan, Prithwish and Fan, Xinyi and Gonzalez-Pumariega, Gonzalo and Kompella, Aditya and Pace, Maximus Adrian and Sharma, Yash and Sun, Xiangwan and Sunkara, Neha and Choudhury, Sanjiban},
  title     = {{MOSAIC}: A Modular System for Assistive and Interactive Cooking},
  booktitle = {Conference on Robot Learning (CoRL)},
  year      = {2024},
  eprint    = {2402.18796},
  archiveprefix = {arXiv},
  primaryclass = {cs.RO}
}


@ARTICLE{Adadi2018,
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE Access}, 
  title={Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}, 
  year={2018},
  volume={6},
  number={},
  pages={52138-52160},
  keywords={Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models},
  doi={10.1109/ACCESS.2018.2870052}}


@article{Gunning2019,
  author    = {Gunning, David and Stefik, Mark and Choi, Jaesik and Miller, Timothy and Stumpf, Simone and Yang, Guang-Zhong},
  title     = {XAI—Explainable artificial intelligence},
  journal   = {Science Robotics},
  volume    = {4},
  number    = {37},
  pages     = {eaay7120},
  year      = {2019}
}

@article{Arrieta2020,
  title     = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  author    = {Arrieta, Alejandro Barredo and D{\'i}az-Rodr{\'i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'i}a, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and others},
  journal   = {Information Fusion},
  volume    = {58},
  pages     = {82--115},
  year      = {2020},
  publisher = {Elsevier}
}

@Article{Asad2024_intention,
AUTHOR = {Asad, Usman and Khalid, Azfar and Lughmani, Waqas Akbar and Rasheed, Shummaila and Khan, Muhammad Mahabat},
TITLE = {A Human Intention and Motion Prediction Framework for Applications in Human-Centric Digital Twins},
JOURNAL = {Biomimetics},
VOLUME = {10},
YEAR = {2025},
NUMBER = {10},
ARTICLE-NUMBER = {656},
URL = {https://www.mdpi.com/2313-7673/10/10/656},
PubMedID = {41149186},
ISSN = {2313-7673},
ABSTRACT = {In manufacturing settings where humans and machines collaborate, understanding and predicting human intention is crucial for enabling the seamless execution of tasks. This knowledge is the basis for creating an intelligent, symbiotic, and collaborative environment. However, current foundation models often fall short in directly anticipating complex tasks and producing contextually appropriate motion. This paper proposes a modular framework that investigates strategies for structuring task knowledge and engineering context-rich prompts to guide Vision–Language Models in understanding and predicting human intention in semi-structured environments. Our evaluation, conducted across three use cases of varying complexity, reveals a critical tradeoff between prediction accuracy and latency. We demonstrate that a Rolling Context Window strategy, which uses a history of frames and the previously predicted state, achieves a strong balance of performance and efficiency. This approach significantly outperforms single-image inputs and computationally expensive in-context learning methods. Furthermore, incorporating egocentric video views yields a substantial 10.7% performance increase in complex tasks. For short-term motion forecasting, we show that the accuracy of joint position estimates is enhanced by using historical pose, gaze data, and in-context examples.},
DOI = {10.3390/biomimetics10100656}
}



@inproceedings{Andronas2021,
  author    = {Andronas, D. and Kokotinis, G. and Makris, S.},
  title     = {On modelling and handling of flexible materials: A review on digital twins and planning systems},
  booktitle = {Procedia CIRP},
  volume    = {97},
  pages     = {447--452},
  year      = {2021}
}

@article{Polini2020,
  author    = {Polini, W. and Corrado, A.},
  title     = {Digital twin of composite assembly manufacturing process},
  journal   = {International Journal of Production Research},
  volume    = {58},
  pages     = {5238--5252},
  month     = {9},
  year      = {2020}
}

@article{Asad2023,
  author    = {Asad, U. and Khan, M. and Khalid, A. and Lughmani, W. A.},
  title     = {Human-centric digital twins in industry: A comprehensive review of enabling technologies and implementation strategies},
  journal   = {Sensors},
  volume    = {23},
  number    = {8},
  year      = {2023}
}

@article{Makris2023,
  author    = {Makris, S. and Dietrich, F. and Kellens, K. and Hu, S.},
  title     = {Automated assembly of non-rigid objects},
  journal   = {CIRP Annals},
  volume    = {72},
  pages     = {513--539},
  month     = {1},
  year      = {2023}
}

@article{Cotta2023,
  author    = {Cotta, W. A. A. and Lopes, S. I. and Vassallo, R. F.},
  title     = {Towards the cognitive factory in industry 5.0: From concept to implementation},
  journal   = {Smart Cities},
  volume    = {6},
  pages     = {1901--1921},
  month     = {8},
  year      = {2023}
}

@article{Zhou2024,
  author    = {Zhou, P. and Zheng, P. and Qi, J. and Li, C. and Lee, H.-Y. and Duan, A. and Lu, L. and Li, Z. and Hu, L. and Navarro-Alarcon, D.},
  title     = {Reactive human-robot collaborative manipulation of deformable linear objects using a new topological latent control model},
  journal   = {Robotics and Computer-Integrated Manufacturing},
  volume    = {88},
  pages     = {102727},
  month     = {8},
  year      = {2024}
}

@article{Sirintuna2024,
  author    = {Sirintuna, D. and Kastritsi, T. and Ozdamar, I. and Gandarias, J. M. and Ajoudani, A.},
  title     = {Enhancing human-robot collaborative transportation through obstacle-aware vibrotactile warning and virtual fixtures},
  journal   = {Robotics and Autonomous Systems},
  pages     = {104725},
  month     = {5},
  year      = {2024}
}

@inproceedings{Nezhad2020,
  author    = {Nezhad, H. Y. and Wang, X. and Court, S. D. and Thapa, B. and Erkoyuncu, J. A.},
  title     = {Development of an augmented reality equipped composites bonded assembly and repair for aerospace applications},
  booktitle = {IFAC-PapersOnLine},
  volume    = {53},
  pages     = {209--215},
  publisher = {Elsevier B.V.},
  year      = {2020}
}

@patent{Laughlin2020,
  author      = {Laughlin, B. D. and Skelton, M. M.},
  title       = {Augmented reality system for manufacturing composite parts},
  number      = {US 16/167,636},
  year        = {2020},
  nationality = {US}
}

@unpublished{Mandlekar2021,
  author  = {Mandlekar, A. and Xu, D. and Wong, J. and Nasiriany, S. and Wang, C. and Kulkarni, R. and Fei-Fei, L. and Savarese, S. and Zhu, Y. and Mart{\'\i}n-Mart{\'\i}n, R.},
  title   = {What matters in learning from offline human demonstrations for robot manipulation},
  note    = {arXiv preprint},
  month   = {8},
  year    = {2021}
}

@unpublished{Chi2023,
  author  = {Chi, C. and Feng, S. and Du, Y. and Xu, Z. and Cousineau, E. and Burchfiel, B. and Song, S.},
  title   = {Diffusion policy: Visuomotor policy learning via action diffusion},
  note    = {arXiv preprint},
  month   = {3},
  year    = {2023}
}

@unpublished{Yu2024,
  author  = {Yu, Q. and Moghani, M. and Dharmarajan, K. and Schorp, V. and Panitch, W. C.-H. and Liu, J. and Hari, K. and Huang, H. and Mittal, M. and Goldberg, K. and Garg, A.},
  title   = {Orbit-surgical: An open-simulation framework for learning surgical augmented dexterity},
  note    = {arXiv preprint},
  month   = {4},
  year    = {2024}
}

@article{Liu2023,
  author    = {Liu, F. and Su, E. and Lu, J. and Li, M. and Yip, M. C.},
  title     = {Robotic manipulation of deformable rope-like objects using differentiable compliant position-based dynamics},
  journal   = {IEEE Robotics and Automation Letters},
  volume    = {8},
  pages     = {3964--3971},
  month     = {7},
  year      = {2023}
}

@unpublished{Wei2022,
  author  = {Wei, J. and Wang, X. and Schuurmans, D. and Bosma, M. and Ichter, B. and Xia, F. and Chi, E. and Le, Q. and Zhou, D.},
  title   = {Chain-of-thought prompting elicits reasoning in large language models},
  note    = {arXiv preprint},
  month   = {1},
  year    = {2022}
}

@article{Fan2024,
  author    = {Fan, H. and Liu, X. and Fuh, J. Y. H. and Lu, W. F. and Li, B.},
  title     = {Embodied intelligence in manufacturing: leveraging large language models for autonomous industrial robotics},
  journal   = {Journal of Intelligent Manufacturing},
  year      = {2024}
}

@unpublished{Li2024,
  author  = {Li, F. and Zhang, R. and Zhang, H. and Zhang, Y. and Li, B. and Li, W. and Ma, Z. and Li, C.},
  title   = {Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models},
  note    = {arXiv preprint arXiv:2407.07895},
  year    = {2024}
}

@article{Singh2023,
  author    = {Singh, I. and Blukis, V. and Mousavian, A. and Goyal, A. and Xu, D. and Tremblay, J. and Fox, D. and Thomason, J. and Garg, A.},
  title     = {Progprompt: program generation for situated robot task planning using large language models},
  journal   = {Autonomous Robots},
  volume    = {47},
  pages     = {999--1012},
  month     = {12},
  year      = {2023}
}

@unpublished{Ahn2022,
  author    = {Ahn, M. and Brohan, A. and Brown, N. and Chebotar, Y. and Cortes, O. and David, B. and Finn, C. and Fu, C. and Gopalakrishnan, K. and Hausman, K. and Herzog, A. and Ho, D. and Hsu, J. and Ibarz, J. and Ichter, B. and Irpan, A. and Jang, E. and Ruano, R. J. and Jeffrey, K. and Jesmonth, S. and Joshi, N. J. and Julian, R. and Kalashnikov, D. and Kuang, Y. and Lee, K.-H. and Levine, S. and Lu, Y. and Luu, L. and Parada, C. and Pastor, P. and Quiambao, J. and Rao, K. and Rettinghouse, J. and Reyes, D. and Sermanet, P. and Sievers, N. and Tan, C. and Toshev, A. and Vanhoucke, V. and Xia, F. and Xiao, T. and Xu, P. and Xu, S. and Yan, M. and Zeng, A.},
  title     = {Do as i can, not as i say: Grounding language in robotic affordances},
  note    = {arXiv preprint},
  month   = {4},
  year    = {2022}
}

@inproceedings{Zheng2023,
  author    = {Zheng, H. and Lee, R. and Lu, Y.},
  title     = {Ha-vid: a human assembly video dataset for comprehensive assembly knowledge understanding},
  booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
  series    = {NIPS '23},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  year      = {2023}
}

@inproceedings{Grauman2024,
  author    = {Grauman, K. and Westbury, A. and Torresani, L. and Kitani, K. and Malik, J. and Afouras, T. and Ashutosh, K. and Baiyya, V. and Bansal, S. and Boote, B. and Byrne, E. and Chavis, Z. and Chen, J. and Cheng, F. and Chu, F.-J. and Crane, S. and Dasgupta, A. and Dong, J. and Escobar, M. and Forigua, C. and Gebreselasie, A. and Haresh, S. and Huang, J. and Islam, M. M. and Jain, S. and Khirodkar, R. and Kukreja, D. and Liang, K. J. and Liu, J.-W. and Majumder, S. and Mao, Y. and Martin, M. and Mavroudi, E. and Nagarajan, T. and Ragusa, F. and Ramakrishnan, S. K. and Seminara, L. and Somayazulu, A. and Song, Y. and Su, S. and Xue, Z. and Zhang, E. and Zhang, J. and Castillo, A. and Chen, C. and Fu, X. and Furuta, R. and Gonz{\'a}lez, C. and Gupta, P. and Hu, J. and Huang, Y. and Huang, Y. and Khoo, W. and Kumar, A. and Kuo, R. and Lakhavani, S. and Liu, M. and Luo, M. and Luo, Z. and Meredith, B. and Miller, A. and Oguntola, O. and Pan, X. and Peng, P. and Pramanick, S. and Ramazanova, M. and Ryan, F. and Shan, W. and Somasundaram, K. and Song, C. and Southerland, A. and Tateno, M. and Wang, H. and Wang, Y. and Yagi, T. and Yan, M. and Yang, X. and Yu, Z. and Zha, S. C. and Zhao, C. and Zhao, Z. and Zhu, Z. and Zhuo, J. and Arbel{\'a}ez, P. and Bertasius, G. and Damen, D. and Engel, J. and Farinella, G. Maria and Furnari, A. and Ghanem, B. and Hoffman, J. and Jawahar, C. V. and Newcombe, R. and Park, H. S. and Rehg, J. M. and Sato, Y. and Savva, M. and Shi, J. and Shout, M. Z. and Wray, M.},
  title     = {Ego-exo4d: Understanding skilled human activity from first- and third-person perspectives},
  booktitle = {2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {19383--19400},
  year      = {2024}
}

@article{Huang2021,
  author    = {Huang, Z. and Hasan, A. and Shin, K. and Li, R. and Driggs-Campbell, K.},
  title     = {Long-term pedestrian trajectory prediction using mutable intention filter and warp lstm},
  journal   = {IEEE Robotics and Automation Letters},
  volume    = {6},
  pages     = {542--549},
  month     = {4},
  year      = {2021}
}

@unpublished{Huang2022,
  author  = {Huang, Z. and Mun, Y.-J. and Li, X. and Xie, Y. and Zhong, N. and Liang, W. and Geng, J. and Chen, T. and Driggs-Campbell, K.},
  title   = {Hierarchical intention tracking for robust human-robot collaboration in industrial assembly tasks},
  note    = {arXiv preprint},
  month   = {3},
  year    = {2022}
}

@article{Mangin2022,
  author    = {Mangin, O. and Roncone, A. and Scassellati, B.},
  title     = {How to be helpful? supportive behaviors and personalization for human-robot collaboration},
  journal   = {Frontiers in Robotics and AI},
  volume    = {8},
  month     = {2},
  year      = {2022}
}

@article{Zhong2024,
  author    = {Zhong, R. and Hu, B. and Hong, Z. and Zhang, Z. and Lou, S. and Song, X. and Feng, Y. and Tan, J.},
  title     = {Human-robot handover task intention recognition framework by fusing human digital twin and deep domain adaptation},
  journal   = {Journal of Engineering Design},
  year      = {2024}
}

@article{Ding2024,
  author    = {Ding, P. and Zhang, J. and Zheng, P. and Zhang, P. and Fei, B. and Xu, Z.},
  title     = {Dynamic scenario-enhanced diverse human motion prediction network for proactive human-robot collaboration in customized assembly tasks},
  journal   = {Journal of Intelligent Manufacturing},
  month     = {7},
  year      = {2024}
}

@unpublished{Mascaro2022,
  author  = {Mascaro, E. V. and Ahn, H. and Lee, D.},
  title   = {Intention-conditioned long-term human egocentric action forecasting},
  note    = {arXiv preprint},
  month   = {7},
  year    = {2022}
}

@unpublished{Grauman2021,
  author  = {Grauman, K. and Westbury, A. and Byrne, E. and Chavis, Z. and Furnari, A. and Girdhar, R. and Hamburger, J. and Jiang, H. and Liu, M. and Liu, X. and Martin, M. and Nagarajan, T. and Radosavovic, I. and Ramakrishnan, S. K. and Ryan, F. and Sharma, J. and Wray, M. and Xu, M. and Xu, E. Z. and Zhao, C. and Bansal, S. and Batra, D. and Cartillier, V. and Crane, S. and Do, T. and Doulaty, M. and Erapalli, A. and Feichtenhofer, C. and Fragomeni, A. and Fu, Q. and Gebreselasie, A. and Gonzalez, C. and Hillis, J. and Huang, X. and Huang, Y. and Jia, W. and Khoo, W. and Kolar, J. and Kottur, S. and Kumar, A. and Landini, F. and Li, C. and Li, Y. and Li, Z. and Mangalam, K. and Modhugu, R. and Munro, J. and Murrell, T. and Nishiyasu, T. and Price, W. and Puentes, P. R. and Ramazanova, M. and Sari, L. and Somasundaram, K. and Southerland, A. and Sugano, Y. and Tao, R. and Vo, M. and Wang, Y. and Wu, X. and Yagi, T. and Zhao, Z. and Zhu, Y. and Arbelaez, P. and Crandall, D. and Damen, D. and Farinella, G. M. and Fuegen, C. and Ghanem, B. and Ithapu, V. K. and Jawahar, C. V. and Joo, H. and Kitani, K. and Li, H. and Newcombe, R. and Oliva, A. and Park, H. S. and Rehg, J. M. and Sato, Y. and Shi, J. and Shou, M. Z. and Torralba, A. and Torresani, L. and Yan, M. and Malik, J.},
  title   = {Ego4d: Around the world in 3,000 hours of egocentric video},
  note    = {arXiv preprint},
  month   = {10},
  year    = {2021}
}

@article{Lemaignan2017,
  author    = {Lemaignan, S. and Warnier, M. and Sisbot, E. A. and Clodic, A. and Alami, R.},
  title     = {Artificial cognition for social human-robot interaction: An implementation},
  journal   = {Artificial Intelligence},
  volume    = {247},
  pages     = {45--69},
  month     = {6},
  year      = {2017}
}

@inproceedings{Guo2022,
  author    = {Guo, C. and Zou, S. and Zuo, X. and Wang, S. and Ji, W. and Li, X. and Cheng, L.},
  title     = {Generating diverse and natural 3d human motions from text},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {5152--5161},
  month     = {June},
  year      = {2022}
}

@article{Peng2021a,
  author    = {Peng, X. B. and Ma, Z. and Abbeel, P. and Levine, S. and Kanazawa, A.},
  title     = {Amp: Adversarial motion priors for stylized physics-based character control},
  journal   = {ACM Trans. Graph.},
  volume    = {40},
  month     = {July},
  year      = {2021}
}

@article{Peng2021b,
  author    = {Peng, X. B. and Ma, Z. and Abbeel, P. and Levine, S. and Kanazawa, A.},
  title     = {Amp: adversarial motion priors for stylized physics-based character control},
  journal   = {ACM Transactions on Graphics},
  volume    = {40},
  pages     = {1--20},
  month     = {8},
  year      = {2021}
}

@article{Peng2022,
  author    = {Peng, X. B. and Guo, Y. and Halper, L. and Levine, S. and Fidler, S.},
  title     = {Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters},
  journal   = {ACM Trans. Graph.},
  volume    = {41},
  month     = {July},
  year      = {2022}
}

@inproceedings{Tessler2023,
  author    = {Tessler, C. and Kasten, Y. and Guo, Y. and Mannor, S. and Chechik, G. and Peng, X. B.},
  title     = {Calm: Conditional adversarial latent models for directable virtual characters},
  booktitle = {Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings},
  pages     = {1--9},
  publisher = {ACM},
  month     = {7},
  year      = {2023}
}

@inproceedings{Tevet2023,
  author    = {Tevet, G. and Raab, S. and Gordon, B. and Shafir, Y. and Cohen-or, D. and Bermano, A. H.},
  title     = {Human motion diffusion model},
  booktitle = {The Eleventh International Conference on Learning Representations},
  year      = {2023}
}

@inproceedings{Yuan2023,
  author    = {Yuan, Y. and Song, J. and Iqbal, U. and Vahdat, A. and Kautz, J.},
  title     = {Physdiff: Physics-guided human motion diffusion model},
  booktitle = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages     = {15964--15975},
  publisher = {IEEE},
  month     = {10},
  year      = {2023}
}

@inproceedings{Tessler2024,
  author    = {Tessler, C. and Guo, Y. and Nabati, O. and Chechik, G. and Peng, X. B.},
  title     = {Maskedmimic: Unified physics-based character control through masked motion},
  booktitle = {ACM Transactions On Graphics (TOG)},
  publisher = {ACM New York, NY, USA},
  year      = {2024}
}

@unpublished{Tevet2024,
  author  = {Tevet, G. and Raab, S. and Cohan, S. and Reda, D. and Luo, Z. and Peng, X. B. and Bermano, A. H. and van de Panne, M.},
  title   = {Closd: Closing the loop between simulation and diffusion for multi-task character control},
  note    = {arXiv preprint arXiv:2410.03441},
  year    = {2024}
}

@unpublished{Ha2023,
  author  = {Ha, H. and Florence, P. and Song, S.},
  title   = {Scaling up and distilling down: Language-guided robot skill acquisition},
  note    = {arXiv preprint},
  month   = {7},
  year    = {2023}
}

@unpublished{Chen2023,
  author  = {Chen, X. and Zhang, S. and Zhang, P. and Zhao, L. and Chen, J.},
  title   = {Asking before action: Gather information in embodied decision making with language models},
  note    = {arXiv preprint},
  month   = {5},
  year    = {2023}
}

@unpublished{Fourney2024,
  author  = {Fourney, A. and Bansal, G. and Mozannar, H. and Tan, C. and Salinas, E. and Zhu, E. and Niedtner, F. and Proebsting, G. and Bassman, G. and Gerrits, J. and Alber, J. and Chang, P. and Loynd, R. and West, R. and Dibia, V. and Awadallah, A. and Kamar, E. and Hosn, R. and Amershi, S.},
  title   = {Magentic-one: A generalist multi-agent system for solving complex tasks},
  note    = {arXiv preprint},
  year    = {2024}
}

@inproceedings{Mahmood2019,
  author    = {Mahmood, N. and Ghorbani, N. and Troje, N. F. and Pons-Moll, G. and Black, M. J.},
  title     = {AMASS: Archive of motion capture as surface shapes},
  booktitle = {International Conference on Computer Vision},
  pages     = {5442--5451},
  month     = {Oct},
  year      = {2019}
}

@article{Plappert_undated,
  author  = {Plappert, M. and Mandery, C. and Asfour, T.},
  title   = {The KIT motion-language dataset},
  journal = {Big Data}
}

@inproceedings{Sener2022,
  author    = {Sener, F. and Chatterjee, D. and Shelepov, D. and He, K. and Singhania, D. and Wang, R. and Yao, A.},
  title     = {Assembly101: A large-scale multi-view video dataset for understanding procedural activities},
  booktitle = {CVPR},
  year      = {2022}
}

@inproceedings{Baradel2024,
  author    = {Baradel, F. and Armando, M. and Galaaoui, S. and Br{\'e}gier, R. and Weinzaepfel, P. and Rogez, G. and Lucas, T.},
  title     = {Multi-hmr: Multi-person whole-body human mesh recovery in a single shot},
  booktitle = {ECCV},
  year      = {2024}
}

@inproceedings{Pavlakos2019,
  author    = {Pavlakos, G. and Choutas, V. and Ghorbani, N. and Bolkart, T. and Osman, A. A. A. and Tzionas, D. and Black, M. J.},
  title     = {Expressive body capture: 3D hands, face, and body from a single image},
  booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {10975--10985},
  year      = {2019}
}

@inproceedings{Shen2024,
  author    = {Shen, Z. and Pi, H. and Xia, Y. and Cen, Z. and Peng, S. and Hu, Z. and Bao, H. and Hu, R. and Zhou, X.},
  title     = {World-grounded human motion recovery via gravity-view coordinates},
  booktitle = {SIGGRAPH Asia Conference Proceedings},
  year      = {2024}
}

@inproceedings{Piccinelli2025,
  author    = {Piccinelli, L. and Sakaridis, C. and Segu, M. and Yang, Y.-H. and Li, S. and Abbeloos, W. and Van Gool, L.},
  title     = {UniK3D: Universal camera monocular 3d estimation},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2025}
}

@unpublished{Ryan2024,
  author  = {Ryan, F. and Bati, A. and Lee, S. and Bolya, D. and Hoffman, J. and Rehg, J. M.},
  title   = {Gaze-lle: Gaze target estimation via large-scale learned encoders},
  note    = {arXiv preprint arXiv:2412.09586},
  year    = {2024}
}

@inproceedings{Tuli2021,
  author    = {Tuli, T. B. and Kohl, L. and Chala, S. A. and Manns, M. and Ansari, F.},
  title     = {Knowledge-based digital twin for predicting interactions in human-robot collaboration},
  booktitle = {2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)},
  volume    = {2021-September},
  pages     = {1--8},
  publisher = {IEEE},
  month     = {9},
  year      = {2021}
}

@unpublished{Wan2025,
  author  = {Wan, T. and Wang, A. and Ai, B. and Wen, B. and Mao, C. and Xie, C.-W. and Chen, D. and Yu, F. and Zhao, H. and Yang, J. and Zeng, J. and Wang, J. and Zhang, J. and Zhou, J. and Wang, J. and Chen, J. and Zhu, K. and Zhao, K. and Yan, K. and Huang, L. and Feng, M. and Zhang, N. and Li, P. and Wu, P. and Chu, R. and Feng, R. and Zhang, S. and Sun, S. and Fang, T. and Wang, T. and Gui, T. and Weng, T. and Shen, T. and Lin, W. and Wang, W. and Wang, W. and Zhou, W. and Wang, W. and Yu, W. and Shi, X. and Huang, X. and Xu, X. and Kou, Y. and Lv, Y. and Li, Y. and Liu, Y. and Wang, Y. and Zhang, Y. and Huang, Y. and Li, Y. and Wu, Y. and Liu, Y. and Pan, Y. and Zheng, Y. and Hong, Y. and Shi, Y. and Feng, Y. and Jiang, Z. and Han, Z. and Wu, Z.-F. and Liu, Z.},
  title   = {Wan: Open and advanced large-scale video generative models},
  note    = {arXiv preprint arXiv:2503.20314},
  year    = {2025}
}

@article{Manyar2023,
  author    = {Manyar, O. M. and Cheng, J. and Levine, R. and Krishnan, V. and Barbi{\v{c}}, J. and Gupta, S. K.},
  title     = {Physics informed synthetic image generation for deep learning-based detection of wrinkles and folds},
  journal   = {Journal of Computing and Information Science in Engineering},
  volume    = {23},
  month     = {6},
  year      = {2023}
}

@book{Manning2008,
  author    = {Manning, C. D. and Raghavan, P. and Sch{\"u}tze, H.},
  title     = {Introduction to Information Retrieval},
  publisher = {Cambridge University Press},
  year      = {2008}
}

@unpublished{Gema2025,
  author  = {Gema, A. P. and H{\"a}gele, A. and Chen, R. and Arditi, A. and Goldman-Wetzler, J. and Fraser-Taliente, K. and Sleight, H. and Petrini, L. and Michael, J. and Alex, B. and Minervini, P. and Chen, Y. and Benton, J. and Perez, E.},
  title   = {Inverse scaling in test-time compute},
  note    = {arXiv preprint},
  year    = {2025}
}

@unpublished{Abeyruwan2022,
  author  = {Abeyruwan, S. and Graesser, L. and D'Ambrosio, D. B. and Singh, A. and Shankar, A. and Bewley, A. and Jain, D. and Choromanski, K. and Sanketi, P. R.},
  title   = {i-sim2real: Reinforcement learning of robotic policies in tight human-robot interaction loops},
  note    = {arXiv preprint},
  month   = {7},
  year    = {2022}
}